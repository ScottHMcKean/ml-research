{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00dffd78-ad95-482f-b361-e5a9bc7cb3e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Tensorflow DataLoad Profiling\n",
    "The following notebooks profile Tensorflow dataloaders and trainers on Databricks with an eye to parallelism and optimization. It uses the CPPE-5 dataset, reconstructed into a typical file-based format and loaded onto Volumes for debugging how files are loaded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "296ca211-32d4-462d-b72c-b7bdc15b94db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Baseline Reproduction\n",
    "This notebook provides a baseline reproduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea1dec9d-a5eb-40e9-a375-a2195314c857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "urllib3.connection.HTTPConnection.default_pool_maxsize = 50\n",
    "urllib3.connection.HTTPSConnection.default_pool_maxsize = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a10f5b5-994b-4313-b029-a86472a1a1c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install opencv-python\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644b4114-363a-4094-a06e-b94c0b64b618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c1a9cf6-ece8-42a6-8512-6364788771ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "These are the core functions of the dataloader - creating a dataset with a _process_path command. Note some of the tensorflow optimizations here: `cache`, `batch`, and `prefetch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f33a82-fe74-4fea-abd7-99ca846ccf6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    image_paths: list, \n",
    "    batch_size: int = 32, \n",
    "    img_shape=(224,224), \n",
    "    use_random=True, \n",
    "    max_objects=30\n",
    "    ):\n",
    "    \"Creates a dataset from a serialized list of loaded data\"\n",
    "    def gen():\n",
    "        for image_path in image_paths:\n",
    "            yield image_path\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_signature=tf.TensorSpec(shape=(), dtype=tf.string)\n",
    "    )\n",
    "    \n",
    "    if use_random:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "    \n",
    "    def _load_and_process_py(image_path_tensor):\n",
    "        \"\"\"Python function to load image and annotations\"\"\"\n",
    "        # Convert tensor to string\n",
    "        image_path = image_path_tensor.numpy().decode('utf-8')\n",
    "        \n",
    "        # Load and process image\n",
    "        v_data = cv2.imread(image_path)\n",
    "        v_data = cv2.resize(v_data, (img_shape[1], img_shape[0])) / 255.0\n",
    "        v_data = v_data.astype(np.float32) #HWC\n",
    "        \n",
    "        # Load and parse annotation\n",
    "        annotation_path = image_path.replace('.png', '.json')\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            ann = json.load(f)\n",
    "        \n",
    "        bboxes = np.array(ann['objects']['bbox'], dtype=np.float32)\n",
    "        categories = np.array(ann['objects']['category'], dtype=np.int32)\n",
    "        \n",
    "        # Pad bboxes and categories\n",
    "        padded_bboxes = np.zeros((max_objects, 4), dtype=np.float32)\n",
    "        padded_categories = np.full((max_objects), -1, dtype=np.int32)\n",
    "        \n",
    "        num_objects = min(len(bboxes), max_objects)\n",
    "        padded_bboxes[:num_objects] = bboxes[:num_objects]\n",
    "        padded_categories[:num_objects] = categories[:num_objects]\n",
    "        \n",
    "        return v_data, padded_bboxes, padded_categories\n",
    "    \n",
    "    def _process_path(image_path):\n",
    "        \"\"\"Wrapper for py_function with proper output signatures\"\"\"\n",
    "        image, bboxes, categories = tf.py_function(\n",
    "            _load_and_process_py,\n",
    "            [image_path],\n",
    "            [tf.float32, tf.float32, tf.int32]\n",
    "        )\n",
    "    \n",
    "        image = tf.ensure_shape(image, (img_shape[0], img_shape[1], 3))\n",
    "        bboxes = tf.ensure_shape(bboxes, (max_objects, 4))\n",
    "        categories = tf.ensure_shape(categories, (max_objects,))\n",
    "        \n",
    "        return {'images': tf.cast(image, tf.float32, name='images')}, {\n",
    "            'bboxes': tf.cast(bboxes, tf.float32, name='bboxes'),\n",
    "            'classes': tf.cast(categories, tf.int32, name='classes')\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(_process_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b77ec865-125a-4f78-822c-73538f5f8984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "image_paths = [str(x) for x in Path('/Volumes/shm/default/cppe5/').glob('*.png')]\n",
    "dataset = create_dataset(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6f25677-5ce8-46ce-b17b-072378f6dde9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can test dataset performance by taking one *batch*. Note the libpng error - this is one argument for using a preprocessing pipeline to create tensorflow native binary files (TFRecords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8efccaa5-1bbc-4629-bf1c-fc03c703fc09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for batch in dataset.take(1):\n",
    "    images = batch[0]['images']\n",
    "    bboxes = batch[1]['bboxes']\n",
    "    categories = batch[1]['classes']\n",
    "    print(f\"Input shape: {images.shape}\")\n",
    "    print(f\"Bounding boxes shape: {bboxes.shape}\")\n",
    "    print(f\"Categories shape: {categories.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a11401-1f3c-4d75-b82a-df33abae86f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from model import build_object_detection_model\n",
    "model = build_object_detection_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7653268c-ffb7-4e6d-9efc-c47c5c93939f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We use the `%%time` operator to time our model fitting. The biggest performance improvement here is the steps_per_epoch, where we ensure that the steps aren't too small. Note the poor GPU performance on this data loader - I think this is likely attributed to the CV2 and image path reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00378b67-6135-4fe7-ada4-885dfd8fb7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# we can adjust steps per epoch to avoid running out at the end, which speeds computation significantly\n",
    "steps_per_epoch = len(image_paths) // 32\n",
    "model.fit(dataset, epochs=1, steps_per_epoch=steps_per_epoch)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_cppe_dataloader_repro.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
