{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34866e3f-8ec2-4b7b-a51a-eb9032a4d755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook converts the dataset into a Ray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644b4114-363a-4094-a06e-b94c0b64b618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "import tensorflow as tf\n",
    "from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea51bdfe-66f1-48f1-a11c-acc749094ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# shutdown_ray_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff267e5f-6ad9-4d7c-8dcb-a66bbced2307",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "setup_ray_cluster(\n",
    "    num_cpus_per_worker=16,\n",
    "    num_gpus_per_worker=0,\n",
    "    max_worker_nodes=2,\n",
    "    num_cpus_head_node=4,\n",
    "    num_gpus_head_node=0,\n",
    ")\n",
    "\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4679369-9c57-4152-934e-d74e0160dc1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea1a2c94-60b2-49f7-989b-6322c67fdc4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "image_paths = [str(x) for x in Path('/Volumes/shm/default/cppe5/').glob('*.png')][0:100]\n",
    "batch_size=32\n",
    "img_shape=(224, 224)\n",
    "MAX_OBJECTS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac499627-4046-4590-8602-d3a154c15d1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from preprocess import load_and_preprocess, prepare_tf_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8027037-e9ac-431b-a637-a4871d9a7145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing in parallel\n",
    "tf_ds = (\n",
    "  ray.data.from_items(image_paths)\n",
    "  .map(load_and_preprocess)\n",
    "  .map_batches(prepare_tf_batch, batch_size=batch_size)\n",
    "  .to_tf(\n",
    "    feature_columns=[\"images\"],\n",
    "    label_columns=[\"bboxes\", \"classes\"],\n",
    "    batch_size=batch_size,\n",
    "    drop_last=True\n",
    "  )\n",
    "  .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b43018a9-8e39-47d5-8c0e-341d3840fd23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tf_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41cd371a-80c0-43fb-8cc7-8aedd53d06ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for batch in tf_ds.take(1):\n",
    "    images = batch[0]['images']\n",
    "    bboxes = batch[1]['bboxes']\n",
    "    categories = batch[1]['classes']\n",
    "    print(f\"Input shape: {images.shape}\")\n",
    "    print(f\"Bounding boxes shape: {bboxes.shape}\")\n",
    "    print(f\"Categories shape: {categories.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ead987-1a23-47a7-8551-de6d163e4e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from model import build_object_detection_model, masked_mse, masked_sparse_categorical_crossentropy, process_predictions\n",
    "\n",
    "model = build_object_detection_model(num_classes=5, max_objects=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00378b67-6135-4fe7-ada4-885dfd8fb7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.fit(tf_ds, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "694d7f37-6648-4bae-a8c2-2ada4aa98a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def train_func(config):\n",
    "    # Use MultiWorkerMirroredStrategy for multi-GPU distributed training\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        # Define a simple model\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(config[\"num_classes\"], activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Mock dataset for demonstration purposes\n",
    "        train_data = tf.random.uniform((config[\"batch_size\"], 128, 128, 3))\n",
    "        train_labels = tf.random.uniform((config[\"batch_size\"],), maxval=config[\"num_classes\"], dtype=tf.int32)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_data, train_labels, epochs=config[\"epochs\"], batch_size=config[\"batch_size\"])\n",
    "\n",
    "    return model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d79bba0d-4dc0-490a-aea5-eb0a21c9e022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Working on a Ray Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7285b7b8-78d5-435e-bec2-ecae512e623b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.train.tensorflow import TensorflowTrainer\n",
    "from ray.train import ScalingConfig\n",
    "\n",
    "def train_func(config):\n",
    "    import tensorflow as tf\n",
    "    from ray.train.tensorflow import prepare_dataset_shard\n",
    "    \n",
    "    # Get the TensorFlow dataset shard for this worker\n",
    "    tf_dataset = ray.train.get_dataset_shard(\"train\")\n",
    "    \n",
    "    # Create your model (same as your existing model)\n",
    "    model = build_object_detection_model(\n",
    "        num_classes=config[\"num_classes\"],\n",
    "        input_shape=config[\"input_shape\"],\n",
    "        max_objects=config[\"max_objects\"]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        tf_dataset,\n",
    "        epochs=config[\"epochs\"],\n",
    "        callbacks=[ray.train.tensorflow.TensorflowCheckpoint.from_checkpoint_dir(\n",
    "            checkpoint_dir=ray.train.get_checkpoint_dir()\n",
    "        )]\n",
    "    )\n",
    "    \n",
    "    # Save the model for later use\n",
    "    ray.train.report({\"status\": \"training_completed\"})\n",
    "    ray.train.save_checkpoint({\"model\": model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "475935bd-b847-443b-8646-f18b074d2668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure training parameters\n",
    "# trainer = TensorflowTrainer(\n",
    "#     train_func,\n",
    "#     train_loop_config={\n",
    "#         \"num_classes\": 10,  # Adjust based on your dataset\n",
    "#         \"input_shape\": (3, 224, 224),\n",
    "#         \"max_objects\": 50,\n",
    "#         \"epochs\": 3,\n",
    "#         \"batch_size\": 32\n",
    "#     },\n",
    "#     scaling_config=ScalingConfig(\n",
    "#         num_workers=2,  # Number of GPUs to use\n",
    "#         use_gpu=False,\n",
    "#         resources_per_worker={\"GPU\": 0, \"CPU\":4}\n",
    "#     ),\n",
    "#     datasets={\"train\": tf_ds}\n",
    "# )\n",
    "\n",
    "# # Start distributed training\n",
    "# result = trainer.fit()\n",
    "\n",
    "# # Get the best checkpoint\n",
    "# best_checkpoint = result.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7356266d-4f84-48e9-b842-944a895d3a8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_cppe_dataloader_ray.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
